{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b545z9l0HS0M"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ben relocated to Paris last year to pursue his passion. His currently enrolled in a comprehensive course on Natural Language Processing\"\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK5KzBOgINwc",
        "outputId": "97e07866-7667-44ef-a418-fed8d5b93e7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ben relocated to Paris last year to pursue his passion. His currently enrolled in a comprehensive course on Natural Language Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oLlfykeKRnq",
        "outputId": "2fcf44a8-6e2d-40e2-f3db-0e01f7a7c343"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmZczjykJ1IY",
        "outputId": "8553c901-bf4e-403e-c9c8-56822e9a2034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ben', 'relocated', 'to', 'Paris', 'last', 'year', 'to', 'pursue', 'his', 'passion', '.', 'His', 'currently', 'enrolled', 'in', 'a', 'comprehensive', 'course', 'on', 'Natural', 'Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmrt = PorterStemmer()\n",
        "stemmed_word = [stemmrt.stem(token) for token in tokens]\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1LSqKzqKqX9",
        "outputId": "60a420b9-9095-4901-cc48-0a101fc4db83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ben', 'reloc', 'to', 'pari', 'last', 'year', 'to', 'pursu', 'hi', 'passion', '.', 'hi', 'current', 'enrol', 'in', 'a', 'comprehens', 'cours', 'on', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## part of speech tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "text_pos_tag = pos_tag(tokens)\n",
        "print(text_pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If3kEucvKsG-",
        "outputId": "09704a34-7cad-4964-8a2c-aa9e1c0f336d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ben', 'NNP'), ('relocated', 'VBD'), ('to', 'TO'), ('Paris', 'NNP'), ('last', 'JJ'), ('year', 'NN'), ('to', 'TO'), ('pursue', 'VB'), ('his', 'PRP$'), ('passion', 'NN'), ('.', '.'), ('His', 'PRP$'), ('currently', 'RB'), ('enrolled', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('comprehensive', 'JJ'), ('course', 'NN'), ('on', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset(\"NNP\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "entities = nltk.ne_chunk(text_pos_tag)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPWoWB9dLj99",
        "outputId": "9c52df9b-319d-46f6-abfe-779adb026e81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Ben/NNP)\n",
            "  relocated/VBD\n",
            "  to/TO\n",
            "  (GPE Paris/NNP)\n",
            "  last/JJ\n",
            "  year/NN\n",
            "  to/TO\n",
            "  pursue/VB\n",
            "  his/PRP$\n",
            "  passion/NN\n",
            "  ./.\n",
            "  His/PRP$\n",
            "  currently/RB\n",
            "  enrolled/VBN\n",
            "  in/IN\n",
            "  a/DT\n",
            "  comprehensive/JJ\n",
            "  course/NN\n",
            "  on/IN\n",
            "  (ORGANIZATION Natural/NNP Language/NNP)\n",
            "  Processing/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained word embedding\n",
        "#!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "## find similarity of word  using pretrained embadding\n",
        "word1 = nlp(\"king\")\n",
        "word2 = nlp(\"queen\")\n",
        "word3 = nlp(\"princess\")\n",
        "similarity1 = word1.similarity(word2)\n",
        "similarity2 = word1.similarity(word3)\n",
        "similarity3 = word1.similarity(word3)\n",
        "print(\"WORD1 and WORD2 SIMILARITY IS {}\".format(similarity1))\n",
        "print(\"WORD1 and WORD3 SIMILARITY IS {}\".format(similarity2))\n",
        "print(\"WORD1 and WORD3 SIMILARITY IS {}\".format(similarity3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At62KKvpMwWn",
        "outputId": "8406a823-e3f2-45ba-c0a9-49e9c8a6453d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD1 and WORD2 SIMILARITY IS 0.6108841628588695\n",
            "WORD1 and WORD3 SIMILARITY IS 0.6533219144275741\n",
            "WORD1 and WORD3 SIMILARITY IS 0.6533219144275741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVG82zqWNQop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5DcOrYdOnAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}